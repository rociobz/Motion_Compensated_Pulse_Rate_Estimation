{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble  import AdaBoostRegressor\n",
    "from sklearn.tree  import DecisionTreeRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl, ref_fl, fs, window_length_s, window_shift_s):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    sig = data[2:]\n",
    "    print(sig.shape)\n",
    "    \n",
    "    ref = scipy.io.loadmat(ref_fl)[\"BPM0\"]\n",
    "    print(ref.shape)\n",
    "    ref = np.array([x[0] for x in ref])\n",
    "    labels, features, signals = [], [], []\n",
    "    \n",
    "    for i in range(0, len(ref)):\n",
    "        shift = fs * window_shift_s\n",
    "        signal_length = fs * window_length_s\n",
    "        start = i*shift\n",
    "        end = start + signal_length\n",
    "        \n",
    "        ppg = sig[0, start:end]            \n",
    "        accx = sig[1, start:end]\n",
    "        accy = sig[2, start:end]\n",
    "        accz = sig[3, start:end]\n",
    "        \n",
    "        feature, ppg, accx, accy, accz = Featurize(ppg, accx, accy, accz, fs)\n",
    "        signals.append([ppg, accx, accy, accz])\n",
    "        labels.append(ref[i])\n",
    "        features.append(feature)\n",
    "    \n",
    "    return (np.array(labels), np.array(features), signals)\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "    Computes the MAE at 90% availability. \n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    fs = 125\n",
    "    window_length_s = 8\n",
    "    window_shift_s = 2    \n",
    "    \n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl, fs, window_length_s, window_shift_s)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "def bandpass_filter(signal, fs):\n",
    "    \"\"\"\n",
    "    It suppresses frequencies outside a band.\n",
    "    Args: \n",
    "        signal: numpy array to filter\n",
    "        fs: sampling frequency (Hz)\n",
    "        low_f: low frequency cutoff (for example: 40/60.0)\n",
    "        high_f: high frequency cutoff (for example: 240/60.0)\n",
    "    Returns:\n",
    "        filtered signal\n",
    "    \"\"\" \n",
    "    b, a = scipy.signal.butter(3, (40/60.0, 240/60.0), btype='bandpass', fs=fs)\n",
    "    return scipy.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def Featurize(ppg, accx, accy, accz, fs):\n",
    "    \"\"\"\n",
    "    Feature extraction from the PPG and IMU (accelerometer) signals.\n",
    "    Args: (numpy arrays)\n",
    "      ppg: PPG signal data\n",
    "      accx: accelerometer x-channel\n",
    "      accy: accelerometer y-channel\n",
    "      accz: accelerometer z-channel\n",
    "      fs: sampling frequency (Hz)\n",
    "    Returns:\n",
    "        n-tuple of features, ppg, accx, accy, accz\n",
    "    \"\"\"\n",
    "    # Bandpass filter the signals\n",
    "    ppg = bandpass_filter(ppg, fs)\n",
    "    accx = bandpass_filter(accx, fs)\n",
    "    accy = bandpass_filter(accy, fs)\n",
    "    accz = bandpass_filter(accz, fs)\n",
    "    \n",
    "    mn_p = np.mean(ppg)\n",
    "    mn_x = np.mean(accx)\n",
    "    mn_y = np.mean(accy)\n",
    "    mn_z = np.mean(accz)\n",
    "    \n",
    "  # Fourier Transform \n",
    "    fft_len = max(len(accx), 2046)   #array of frequency bins\n",
    "    fft_freqs = np.fft.rfftfreq(fft_len, 1 / fs)\n",
    "    freqs_bw = lambda low, high: (fft_freqs >= low) & (fft_freqs <= high) #select frequency bins\n",
    "  # Accelerometer magnitude\n",
    "    accm = np.sqrt(np.sum(np.square(np.vstack((accx, accy, accz))), axis=0))\n",
    "  # FFT of the accelerometer signal\n",
    "    fft_x = np.fft.rfft(accx, fft_len)\n",
    "    fft_y = np.fft.rfft(accy, fft_len)\n",
    "    fft_z = np.fft.rfft(accz, fft_len)\n",
    "    fft_m = np.fft.rfft(accm, fft_len)\n",
    "  # Energy spectrum\n",
    "    spec_energy_x = np.square(np.abs(fft_x))\n",
    "    spec_energy_y = np.square(np.abs(fft_y))\n",
    "    spec_energy_z = np.square(np.abs(fft_z))\n",
    "    spec_energy_m = np.square(np.abs(fft_m))\n",
    "  # Maximum frequency between 0.25-12 Hz\n",
    "    dom_x = fft_freqs[np.argmax(fft_x[freqs_bw(0.25, 12)])]\n",
    "    dom_y = fft_freqs[np.argmax(fft_y[freqs_bw(0.25, 12)])]\n",
    "    dom_z = fft_freqs[np.argmax(fft_z[freqs_bw(0.25, 12)])]\n",
    "    dom_m = fft_freqs[np.argmax(fft_m[freqs_bw(0.25, 12)])]\n",
    "    \n",
    "    return (np.array([mn_p, mn_x, mn_y, mn_z, dom_x, dom_y, dom_z, dom_m]), \n",
    "            ppg, accx, accy, accz)\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl, fs = 125, window_length_s = 8, window_shift_s = 2):\n",
    "    \"\"\" \n",
    "    Compute mean absolute error (MAE) and confidence from predictions.\n",
    "    Args:\n",
    "      data_fl: signal data\n",
    "      ref_fl: reference data\n",
    "      fs: sampling frequency (Hz)\n",
    "      window_length_s: window size, signal length in seconds\n",
    "      window_shift_s: signal shift in seconds\n",
    "    Returns:\n",
    "        numpy arrays for MAE between prediction-ground truth, and confidence about predictions.\n",
    "    \"\"\"\n",
    "    # Load data using LoadTroikaDataFile   \n",
    "    labels, features, signals = LoadTroikaDataFile(data_fl, ref_fl, fs, window_length_s, window_shift_s)\n",
    "    error, confidence = [], []\n",
    "    model = load_model()\n",
    "    \n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "    for i,feature in enumerate(features):\n",
    "        pred = model.predict(np.reshape(feature, (1, -1)))[0]\n",
    "        sum = np.sum(feature)\n",
    "        sum_f = feature[0]*1.20 + feature[5]*1.1 + feature[7]*1.85\n",
    "        conf = abs(sum_f/sum)\n",
    "        error.append(np.abs((pred-labels[i])))\n",
    "        confidence.append(conf)\n",
    "    # Return per-estimate mean absolute error and confidence as a 2-tuple of numpy arrays.\n",
    "    return np.array(error), np.array(confidence)\n",
    "\n",
    "def train(fs=125, window_length_s=8, window_shift_s=2):\n",
    "    \"\"\" \n",
    "    Model training using Adaboost Regressor.\n",
    "    Args:\n",
    "      fs: sampling frequency (Hz)\n",
    "      window_length_s: window size, signal length in seconds\n",
    "      window_shift_s: signal shift in seconds\n",
    "    Returns:\n",
    "        trained model\n",
    "    \"\"\"\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    labels, features, signals = [], [], []\n",
    "    \n",
    "    for data_fl, ref_fl in (zip(data_fls, ref_fls)):\n",
    "        label, feature, signal = LoadTroikaDataFile(data_fl, ref_fl, fs, window_length_s, window_shift_s)\n",
    "        labels.extend(label)\n",
    "        features.extend(feature)\n",
    "        signals.extend(signal)\n",
    "                            \n",
    "    labels = np.array(labels)\n",
    "    features = np.array(features)\n",
    "    \n",
    "    model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=10), learning_rate=0.001, n_estimators=300, random_state=42)\n",
    "    kFolds = KFold(n_splits=8)\n",
    "    splits = kFolds.split(features,labels)\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        X_train, y_train = features[train_idx], labels[train_idx]\n",
    "        X_test, y_test = features[test_idx], labels[test_idx]\n",
    "        model.fit(X_train, y_train)        \n",
    "    pkl_filename = 'model.pkl'    \n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file) #saves the model in pickle format\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    It loads the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    pkl_filename = 'model.pkl'\n",
    "    pickle_model = None\n",
    "    if (not os.path.isfile(pkl_filename)):\n",
    "        train()\n",
    "    \n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        pickle_model = pickle.load(file)\n",
    "    return pickle_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a description of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Description** - In the previous code, we have developed a pulse rate algorithm that estimates pulse rate from a PPG signal (it optically measures blood flow at the wrist) and a IMU signal (3-axis accelerometer) embedded inside the device to measure motion (linear acceleration). \n",
    "\n",
    "To run the code and use the algorithm, we must first read the data from the Troika dataset with the LoadTroikaDataset() and LoadTroikaDataFile(). The next step is the preprocessing of the signals, which is performed by the bandpass_filter() and Featurize(). RunPulseRateAlgorithm() and train() are the main part of the code, where we develop and train our model. Finally, load_model(), AggregateErrorMetric(), and Evaluate() will trigger the execution of the algorithm and evaluate it's effectiveness.\n",
    "\n",
    "**Data Description** - The data that was used to train and test the algorithm comes from the Troika dataset [1]. Troika includes experimental results on datasets recorded from 12 subjects during fast running at the peak speed of 15 km/hour. This dataset includes two data files for each subject: the data file and the reference file. These files are read in the algorithm by the LoadTroikaDataset() and LoadTroikaDataFile() respectively. The data comes from two different sensors (PPG and accelerometer) recording at 8Hz or 125ms time frames, and in total it contains 4 arrays of data (1 for the PPG and 3 for the accelerometer - x, y, and z axis). \n",
    "\n",
    "One major limitation of this dataset is the lack of demographic data. Due to this, it is not possible to make conclusions regarding gender and age. This is important since the pulse rate varies significantly regarding gender and age. With large enough datasets and more informative clinical metadata (for example, age and gender), wearables can be used to discover previously unknown phenomena that may advance clinical science.\n",
    "\n",
    "[1] Zhilin Zhang, Zhouyue Pi, and Benyuan Liu. TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise. IEEE Trans. on Biomedical Engineering. 2015. DOI: 10.1109/TBME.2014.2359372.\n",
    "\n",
    "**Algorithm Description** - The algorithm works on data form a PPG sensor and an accelerometer. PPG is an optical sensor with LEDs (shine a typically green light into the skin) and a photodetector (measures the reflected light). During the systole phase, when the ventricles contract and pump blood through the arteries, there are more blood cells that absorb the green light so there is less reflected light. On the contrary, during the diastole phase, when the heart relaxes and fills with blood, there are less blood cells and the photodetector sees an increase in reflected light. This oscillating waveform can be used to detect pulse rate.\n",
    "However, during walking or running, we see another periodic signal in the PPG due to arm motion. We use the accelerometer signal of our wearable device to help us keep track of which periodic signal is caused by motion. Because the accelerometer is only sensing arm motion, any periodic signal in the accelerometer is likely not due to the heart beating, and only due to the arm motion. If our pulse rate estimator is picking a frequency that's strong in the accelerometer, it may be making a mistake.\n",
    "\n",
    "The algorithm works following the next steps:\n",
    "1. Loading data: LoadTroikaDataset(), LoadTroikaDataFile()\n",
    " - Load the data and return numpy arrays for PPG and accelerometer signals.\n",
    "2. Preprocessing and build features: bandpass_filter(), Featurize()\n",
    " - We filter the signals using a bandpass filter that suppresses the frequencies outside a specific band. We use a passband (band of frequencies that we want to preserve) between 40 and 240 BPM (beats per minute) because that is the usual frequency range of the pulse rate. \n",
    " - Calculate the magnitude of the acceleration.\n",
    " - Use Fourier Transform to transfer the magnitude to the frequency domain.\n",
    " - Pick the maximum acceleration in the frame as a main feature to the model.\n",
    "3. Build a model: RunPulseRateAlgorithm(), train()\n",
    " - RunPulseRateAlgorithm() uses a window length of 8 seconds and produces an output at least every 2 seconds. \n",
    " - RunPulseRateAlgorithm() returns the per-estimate mean absolute error (MAE) and an estimation confidence. In pulse rate estimation, having a confidence value can be useful if a user wants just a handful of high-quality pulse rate estimate per night. They can use the confidence algorithm to select the 20 most confident estimates at night and ignore the rest of the outputs.  A higher confidence value means that this estimate should be more accurate than an estimate with a lower confidence value. If our algorithm is picking a strong frequency component that's not present in the accelerometer, we can be relatively confident in the estimate. \n",
    " - In train() we use a Decision Tree Regression with AdaBoost and K-Fold Cross Validation to train our model. It returns the model in pickle format.\n",
    "4. Evaluate the model: AggregateErrorMetric(), Evaluate()\n",
    " - Evaluate() uses MAE and confidence returned from RunPulseRateAlgorithm() and runs the AggregateErrorMetric(), which computes the MAE at 90% availability.\n",
    "\n",
    "**Algorithm Performance** - Performance was computed by using cross-validation, specifically K-fold cross-validation. In K-fold cross-validation, the training set is randomly split into K (usually between 5 to 10) subsets known as folds. Where K-1 folds are used to train the model and the other fold is used to test the model. This technique improves the high variance problem in a dataset as we are randomly selecting the training and test folds. \n",
    "\n",
    "The most relevant metric for users of the algorithm is the mean absolute error (MAE), which measures the average of the absolute difference between each ground truth and the predictions. The result of MAE must be less than 15 BPM for the algorithm's performance to pass. \n",
    "\n",
    "This algorithm would need further signal data (it only has data from 12 subjects), demographics data (to distinguish between genders and ages), analysis of errors and confidence evaluation to be generalizable on different datasets. The algorithm design would depend on the application, since how much error is tolerable depends on that. For example, if we were using these pulse rate estimates to compute long term trends over months, then we may be more robust to higher error variance. However, if we wanted to give information back to the user about a specific workout or night of sleep, we would require a much lower error. Therefore, although this algorithm is not generalizable to other datasets, it could be used as a starting point for building a general one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
